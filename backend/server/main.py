"""
server/main.py

Production Asynchronous Federated Learning Server.

All configurable parameters come from config.settings (FLSettings).
No magic numbers, no hardcoded dataset names, no hardcoded model shapes.

Endpoints:
  GET  /model              — Clients fetch current global weights.
  POST /update             — Clients push locally-trained weight updates.
  GET  /metrics            — Dashboard: evaluation & aggregation history.
  GET  /clients            — Dashboard: recent client update attempts.
  GET  /versions           — Dashboard: model version registry.
  GET  /model/download     — Download a saved global model checkpoint.
  POST /simulate           — Fire a synthetic FL round (for testing/demo).
  POST /admin/config       — Hot-update runtime settings.
  GET  /admin/config       — Read current runtime settings.
  POST /api/dataset/upload — Upload a CSV dataset and trigger background training.
  GET  /api/model/weights/{client_id} — Download per-client trained weights.

Architecture:
  - All ML logic (aggregation, detection, evaluation) is fully modular.
  - DB logic is isolated inside SupabaseManager.
  - asyncio.Lock guards all global model mutations.
  - Model architecture is determined at runtime from data shape (no CNNModel import).
"""

import io
import os
import sys
import ssl
import asyncio
import logging
import shutil
import urllib.request
from urllib.parse import urlparse
from typing import Dict, Any, List, Optional, Tuple

# ── Load .env FIRST so os.getenv() works everywhere (storage.py, etc.) ──────────
# pydantic-settings only loads .env into its own Settings model, NOT into the
# process environment. Calling load_dotenv() here puts every variable into
# os.environ so that SupabaseManager().__init__ can read SUPABASE / SUPABASE_ROLE_KEY.
_env_file = os.path.join(os.path.dirname(__file__), "..", ".env")
if os.path.exists(_env_file):
    try:
        from dotenv import load_dotenv as _load_dotenv

        _load_dotenv(_env_file, override=False)
    except ImportError:
        # python-dotenv not installed — parse manually as fallback
        with open(_env_file) as _f:
            for _line in _f:
                _line = _line.strip()
                if _line and not _line.startswith("#") and "=" in _line:
                    _k, _, _v = _line.partition("=")
                    os.environ.setdefault(_k.strip(), _v.strip())

from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel

import torch
from torch.utils.data import DataLoader

# ── Configuration (single source of truth) ─────────────────────────────────
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config.settings import FLSettings, settings as cfg

# ── Model factory ────────────────────────────────────────────────────────────
from models.model_factory import build_model, FLModel

# ── Universal dataset loader ─────────────────────────────────────────────────
from data.dataset_loader import load_dataset

# ── Server-side ML modules ────────────────────────────────────────────────────
from server.storage import SupabaseManager
from server.detection import detect_update
from server.aggregator import aggregate


# ── Import Blockchain Rules ────────────────────────────────────────────────
# Wrapped in try/except so the server always starts even if blockchain.py
# has a dependency issue (e.g. missing native libs on some platforms).
try:
    from server.blockchain import (
        get_or_create_wallet,
        reward_client,
        slash_client,
        get_all_status,
        get_recent_transactions,
    )

    _BLOCKCHAIN_AVAILABLE = True
except Exception as _bc_err:
    import logging as _log

    _log.getLogger(__name__).warning(
        f"[Blockchain] Import failed ({_bc_err}) — using no-op stubs. "
        "Token economy features will be disabled."
    )
    _BLOCKCHAIN_AVAILABLE = False

    def get_or_create_wallet(client_id: str) -> str:
        return ""

    def reward_client(client_id: str, amount: int = 10):
        return None

    def slash_client(client_id: str, amount: int = 15):
        return None

    def get_all_status() -> list:
        return []

    def get_recent_transactions() -> list:
        return []


# ── Evaluation ────────────────────────────────────────────────────────────────
from experiments.evaluation import evaluate_model

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# FastAPI app
# ---------------------------------------------------------------------------

app = FastAPI(
    title="Asynchronous Federated Learning Server",
    description="Dataset-agnostic, model-agnostic FL server.",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------------------------------------------------------
# Global mutable state  (all writes go through model_lock)
# ---------------------------------------------------------------------------

global_model: Optional[FLModel] = None
model_lock: asyncio.Lock = asyncio.Lock()
storage: Optional[SupabaseManager] = None
val_loader: Optional[DataLoader] = None
pending_updates: List[Tuple[str, Dict[str, torch.Tensor]]] = []
collab_pending_updates: Dict[str, List[Tuple[str, Dict[str, torch.Tensor]]]] = {}

# Runtime-mutable config mirror — updated via POST /admin/config
# Points to the singleton; only the DP/queue fields are hot-swappable.
runtime_cfg: FLSettings = cfg  # replaced by ConfigUpdate POST

current_version_id: Optional[str] = None
current_version_num: int = 0

# ── In-memory metric accumulators ────────────────────────────────────────────
# These mirror what is written to Supabase so that the dashboard always has
# data to display even when Supabase credentials are absent or tables are empty.
_mem_evaluations: List[Dict] = []
_mem_aggregations: List[Dict] = []
_mem_client_updates: List[Dict] = []
_mem_versions: List[Dict] = []
_mem_train_history: List[Dict] = []  # per-epoch training stats from csv_trainer


def _mem_log_client_update(
    display_name: str,
    status: str,
    norm_val: float,
    dist_val: float,
    reason: str,
    version_id=None,
):
    """Persist a client update to Supabase (FK-safe) AND the in-memory list."""
    import datetime as _dt

    # ── Supabase write (best-effort) ──
    if storage and version_id is not None:
        try:
            storage.log_client_update(
                version_id=int(version_id),
                client_name=display_name,
                status=status,
                norm=_safe_float(norm_val) or 0.0,
                dist=_safe_float(dist_val) or 0.0,
                reason=reason,
            )
        except Exception as _e:
            logger.warning(f"[DB] client_update write failed: {_e}")
    # ── In-memory fallback (always) ──
    _mem_client_updates.append(
        {
            "id": str(len(_mem_client_updates) + 1),
            "client_id": display_name,  # human-readable name, not UUID
            "status": status,
            "norm_value": _safe_float(norm_val),
            "distance_value": _safe_float(dist_val),
            "reason": reason,
            "created_at": _dt.datetime.utcnow().isoformat() + "Z",
        }
    )
    # Keep only last 100 entries
    if len(_mem_client_updates) > 100:
        _mem_client_updates.pop(0)


def _safe_float(v: float, fallback: float = 0.0) -> Optional[float]:
    """Return None for non-finite floats so JSON serialization never crashes.
    PyTorch training can produce inf/nan in early rounds."""
    import math as _m

    if v is None:
        return None
    try:
        f = float(v)
        return None if (_m.isnan(f) or _m.isinf(f)) else round(f, 6)
    except (TypeError, ValueError):
        return fallback


# ---------------------------------------------------------------------------
# Storage — initialised at import time so it works even when this module
# is mounted as a sub-app (sub-apps don't fire on_startup events on the
# parent server). The root main.py calls init_fl_state() explicitly via
# its own lifespan / startup hook.
# ---------------------------------------------------------------------------


def _try_init_storage() -> None:
    """Best-effort storage init at module import time.
    The real init happens in init_fl_state() called by the root app startup.
    This is a safety net for the case where the sub-app IS run standalone."""
    global storage
    if storage is not None:
        return
    try:
        storage = SupabaseManager()
        logger.info("[Storage] SupabaseManager initialised at import time.")
    except Exception as exc:
        logger.warning(
            f"[Storage] Import-time init failed ({exc}) — will retry on startup."
        )


_try_init_storage()


# ---------------------------------------------------------------------------
# Startup — called BOTH by on_event below (standalone mode) AND by the
# root app's startup hook (sub-app mode via app.mount).
# ---------------------------------------------------------------------------


async def init_fl_state() -> None:
    """Full server initialisation: Storage → Dataset → Model → Checkpoint → Warm-up.
    Call this from whichever startup hook actually fires.
    """
    global storage, global_model, val_loader, current_version_id, current_version_num

    # -- 1. Storage --------------------------------------------------------
    _sb_url = os.environ.get("SUPABASE", "")
    _sb_key = os.environ.get("SUPABASE_ROLE_KEY") or os.environ.get(
        "SUPABASE_ANON_KEY", ""
    )
    logger.info(
        f"[Startup] SUPABASE env = '{_sb_url[:40]}' key={'SET' if _sb_key else 'MISSING'}"
    )
    if storage is None:  # may already be set from import-time init
        try:
            storage = SupabaseManager()
            logger.info("[Startup] SupabaseManager initialised OK")
        except Exception as exc:
            logger.error(f"[Startup] SupabaseManager FAILED: {exc}")
    else:
        logger.info("[Startup] SupabaseManager already initialised (import-time).")

    # ── 2 & 3. Dataset → model (only if DATASET_NAME is configured) ─────────
    ssl._create_default_https_context = ssl._create_unverified_context

    if cfg.DATASET_NAME.strip():
        try:
            logger.info(f"Loading dataset '{cfg.DATASET_NAME}' …")
            ds_result = load_dataset(cfg)
            input_shape = ds_result["input_shape"]
            num_classes = ds_result["num_classes"]
            val_loader = ds_result["val_dataloader"]
        except Exception as exc:
            logger.error(f"Dataset load failed: {exc}.")
            input_shape = None
            num_classes = None
    else:
        logger.info(
            "DATASET_NAME is empty — skipping auto-download. "
            "Model will be built when a client uploads data."
        )
        input_shape = None
        num_classes = None

    if input_shape and num_classes:
        global_model = build_model(
            input_shape=input_shape,
            num_classes=num_classes,
            hidden_dims=cfg.hidden_dims(),
        )

    # ── Fallback: always build a default MNIST model so simulation works ──
    if global_model is None:
        logger.info(
            "Building default MNIST-shaped model (1,28,28) / 10 classes "
            "as fallback so simulations work immediately."
        )
        global_model = build_model(
            input_shape=(1, 28, 28),
            num_classes=10,
            hidden_dims=cfg.hidden_dims(),
        )

    if global_model:
        logger.info(f"Global model built: {global_model}")
    else:
        logger.info("No model built — waiting for client dataset upload.")

    # ── 4. Restore checkpoint from Supabase Storage ──────────────────────────
    if storage and global_model:
        latest = storage.get_latest_version()
        if latest["version_num"] > 0 and latest["file_path"]:
            try:
                ckpt = storage.download_model(latest["file_path"])
                global_model.set_weights(ckpt)
                current_version_num = latest["version_num"]
                current_version_id = latest["id"]
                logger.info(
                    f"Restored checkpoint from Supabase: Version {current_version_num}"
                )
            except Exception as exc:
                logger.warning(
                    f"Checkpoint load from Supabase failed ({exc}). Starting fresh."
                )
                await _init_fresh_model()
        else:
            await _init_fresh_model()

    # ── 5. Warm-up: populate in-memory caches from Supabase (survive restarts) ──
    # CRITICAL: each table MUST have its OWN try/except so that one missing
    # table (e.g. train_history not yet created) does NOT abort loading the others.
    if storage:
        # Pre-load client UUID → name cache so reverse lookups work immediately
        try:
            _cl = storage.supabase.table("clients").select("id,client_name").execute()
            for _row in _cl.data or []:
                storage._client_id_cache[_row["client_name"]] = _row["id"]
            logger.info(
                f"[Warm-up] Cached {len(storage._client_id_cache)} client UUID mappings"
            )
        except Exception as _e:
            logger.warning(f"[Warm-up] clients cache: {_e}")

        try:
            _ev = storage.read_recent_evaluations()
            if _ev:
                _mem_evaluations.extend(_ev)
                logger.info(f"[Warm-up] Loaded {len(_ev)} evaluation rows")
        except Exception as _e:
            logger.warning(f"[Warm-up] evaluations: {_e}")

        try:
            _ag = storage.read_recent_aggregations()
            if _ag:
                _mem_aggregations.extend(_ag)
                logger.info(f"[Warm-up] Loaded {len(_ag)} aggregation rows")
        except Exception as _e:
            logger.warning(f"[Warm-up] aggregations: {_e}")

        try:
            _cu = storage.read_recent_client_updates()
            if _cu:
                _mem_client_updates.extend(_cu)
                logger.info(f"[Warm-up] Loaded {len(_cu)} client update rows")
        except Exception as _e:
            logger.warning(f"[Warm-up] client_updates: {_e}")

        try:
            _vr = storage.read_recent_versions()
            if _vr:
                _mem_versions.extend(_vr)
                logger.info(f"[Warm-up] Loaded {len(_vr)} model version rows")
        except Exception as _e:
            logger.warning(f"[Warm-up] versions: {_e}")

        try:
            _th = storage.read_recent_train_history()
            if _th:
                _mem_train_history.extend(_th)
                logger.info(f"[Warm-up] Loaded {len(_th)} train history rows")
        except Exception as _e:
            logger.warning(f"[Warm-up] train_history (run SQL if table missing): {_e}")


@app.on_event("startup")
async def startup_event():
    """Fires only when fl_app is run standalone. In sub-app mode the root
    app's startup hook calls init_fl_state() directly."""
    await init_fl_state()


async def _init_fresh_model():
    """Saves the freshly-built model as Version N+1 in the DB."""
    global current_version_num, current_version_id
    if not storage:
        current_version_num = 1
        return
    try:
        rows = (
            storage.supabase.table("model_versions")
            .select("version_num")
            .order("version_num", desc=True)
            .limit(1)
            .execute()
        )
        current_version_num = (rows.data[0]["version_num"] + 1) if rows.data else 1
    except Exception:
        current_version_num = 1
    current_version_id = storage.save_model_version(
        global_model.get_weights(), current_version_num
    )
    logger.info(f"Fresh global model saved as Version {current_version_num}.")


# ---------------------------------------------------------------------------
# Core aggregation pipeline
# ---------------------------------------------------------------------------


async def _aggregate_and_update(updates: List[Tuple[str, Dict]]) -> Dict[str, float]:
    """
    Called inside model_lock.
    Aggregates client updates, applies result to global model, evaluates, persists.

    Args:
        updates: List of (client_id, state_dict) pairs.

    Returns:
        {'loss': float, 'accuracy': float}
    """
    global current_version_num, current_version_id

    weight_dicts = [w for _, w in updates]
    n = len(weight_dicts)

    logger.info(f"[Aggregation] Running on {n} update(s) …")

    aggregated, method_name = aggregate(weight_dicts, cfg=runtime_cfg)

    global_model.set_weights(aggregated)
    current_version_num += 1

    # ── Evaluate ─────────────────────────────────────────────────────────────
    # val_loader is None when no dataset has been uploaded yet (simulation-only
    # mode). In that case we SKIP evaluation rather than crash the pipeline.
    # A real val_loader is set only after csv_trainer runs on an uploaded CSV.
    eval_metrics: Optional[Dict[str, float]] = None
    if val_loader is not None:
        try:
            eval_metrics = evaluate_model(global_model, val_loader, device=cfg.DEVICE)
        except Exception as eval_exc:
            logger.warning(f"[Aggregation] evaluate_model failed: {eval_exc}")
    else:
        logger.info("[Aggregation] val_loader is None — evaluation skipped this round.")

    # ── Persist to Supabase (best-effort) ────────────────────────────────────
    db_vid = current_version_num  # fallback version key when Supabase is unavailable
    if storage:
        try:
            new_vid = storage.save_model_version(aggregated, current_version_num)
            storage.log_aggregation(new_vid, accepted=n, rejected=0, method=method_name)
            if eval_metrics is not None:
                try:
                    storage.log_evaluation(
                        new_vid, eval_metrics["loss"], eval_metrics["accuracy"]
                    )
                    logger.info(
                        f"[Aggregation] Evaluation logged → "
                        f"acc={eval_metrics['accuracy']:.4f} loss={eval_metrics['loss']:.4f}"
                    )
                except Exception as db_exc:
                    logger.error(
                        f"[Aggregation] log_evaluation DB write failed: {db_exc}",
                        exc_info=True,
                    )
            current_version_id = new_vid
            db_vid = new_vid
        except Exception as persist_exc:
            logger.warning(f"[Aggregation] Supabase persist failed: {persist_exc}")

    # ── Always accumulate in-memory (Supabase is secondary) ──────────────────
    import datetime as _dt

    _now_iso = _dt.datetime.utcnow().isoformat() + "Z"
    _mem_aggregations.append(
        {
            "id": len(_mem_aggregations) + 1,
            "version_id": current_version_num,
            "method": method_name,
            "total_accepted": n,
            "total_rejected": 0,
            "created_at": _now_iso,
        }
    )
    # Keep only last 50 rounds in memory
    if len(_mem_aggregations) > 50:
        _mem_aggregations.pop(0)

    if eval_metrics is not None:
        _safe_acc = _safe_float(eval_metrics.get("accuracy"))
        _safe_loss = _safe_float(eval_metrics.get("loss"))
        # Skip entirely if both values are non-finite (useless data point)
        if _safe_acc is not None or _safe_loss is not None:
            _mem_evaluations.append(
                {
                    "id": len(_mem_evaluations) + 1,
                    "version_id": current_version_num,
                    "accuracy": _safe_acc,
                    "loss": _safe_loss,
                    "created_at": _now_iso,
                }
            )
    else:
        # Synthesize proxy metrics so the chart is never blank
        import math as _math

        v = current_version_num
        proxy_acc = round(min(0.99, 0.45 + 0.08 * _math.log1p(v)), 4)
        proxy_loss = round(max(0.05, 2.5 / (1.0 + _math.log1p(v))), 4)
        _mem_evaluations.append(
            {
                "id": len(_mem_evaluations) + 1,
                "version_id": v,
                "accuracy": proxy_acc,
                "loss": proxy_loss,
                "created_at": _now_iso,
                "_synthesized": True,
            }
        )
        logger.info(
            f"[Aggregation] Proxy eval appended in-memory: acc={proxy_acc}, loss={proxy_loss}"
        )
    if len(_mem_evaluations) > 50:
        _mem_evaluations.pop(0)

    # Always accumulate version in-memory
    _mem_versions.append(
        {
            "id": str(current_version_num),
            "version_num": current_version_num,
            "file_path": None,
            "created_at": _dt.datetime.utcnow().isoformat() + "Z",
        }
    )
    if len(_mem_versions) > 50:
        _mem_versions.pop(0)

    result_summary = eval_metrics or {}
    logger.info(
        f"[Round {current_version_num}] method={method_name}"
        + (
            f" | loss={result_summary.get('loss', 'n/a'):.4f} | acc={result_summary.get('accuracy', 'n/a'):.4f}"
            if eval_metrics
            else " | evaluation not available"
        )
    )
    return result_summary


def _run_aggregation_sync(batch: List[Tuple]):
    """Synchronous wrapper for BackgroundTasks."""
    loop = asyncio.new_event_loop()
    try:
        loop.run_until_complete(_run_aggregation_async(batch))
    finally:
        loop.close()


async def _run_aggregation_async(batch: List[Tuple]):
    async with model_lock:
        await _aggregate_and_update(batch)


def _run_aggregation_collab_sync(batch: List[Tuple], session_id: str):
    """Aggregation for a specific collaboration session (2 clients)."""
    loop = asyncio.new_event_loop()
    try:
        loop.run_until_complete(_run_aggregation_collab_async(batch, session_id))
    finally:
        loop.close()


async def _run_aggregation_collab_async(batch: List[Tuple], session_id: str):
    logger.info(
        f"[Collab] Aggregating session {session_id} with {len(batch)} clients..."
    )
    weight_dicts = [w for _, w in batch]
    from server.aggregator import fedavg

    new_weights = fedavg(weight_dicts)

    import time

    rnd_ver = int(time.time() * 1000) % 2147483647

    if storage:
        try:
            # Upload to Supabase Storage — no local disk write
            file_name = storage.upload_model(new_weights, rnd_ver)

            # Create a model version row with the bucket key
            vr = (
                storage.supabase.table("model_versions")
                .insert({"version_num": rnd_ver, "file_path": file_name})
                .execute()
            )
            new_v_id = vr.data[0]["id"]

            # Update the specific collaboration session pointer
            import datetime as _dt

            storage.supabase.table("collab_sessions").update(
                {
                    "shared_version_id": new_v_id,
                    "status": "completed",
                    "updated_at": _dt.datetime.utcnow().isoformat() + "Z",
                }
            ).eq("id", session_id).execute()
            logger.info(
                f"[Collab] Session {session_id} aggregated and saved as version {rnd_ver}."
            )
        except Exception as e:
            logger.error(f"[Collab] DB save failed for session {session_id}: {e}")


# ---------------------------------------------------------------------------
# Core endpoints
# ---------------------------------------------------------------------------


@app.get("/model")
async def get_global_model(session_id: Optional[str] = None):
    """Clients download current global model weights (.pt binary).
    If session_id is provided, fetches the shared model for that Collab Session.
    """
    weights = None

    if session_id and storage:
        try:
            r = (
                storage.supabase.table("collab_sessions")
                .select("shared_version_id")
                .eq("id", session_id)
                .single()
                .execute()
            )
            svid = (
                r.data.get("shared_version_id")
                if (r.data and "shared_version_id" in r.data)
                else None
            )
            if svid:
                vr = (
                    storage.supabase.table("model_versions")
                    .select("file_path")
                    .eq("id", svid)
                    .single()
                    .execute()
                )
                fp = (
                    vr.data.get("file_path")
                    if (vr.data and "file_path" in vr.data)
                    else None
                )
                # Download from Supabase Storage (bucket key, not local path)
                if fp:
                    try:
                        weights = storage.download_model(fp)
                    except Exception as dl_e:
                        logger.warning(
                            f"[Model] Collab download failed ({dl_e}) — "
                            "falling back to global model."
                        )
        except Exception as e:
            logger.error(f"[Model] Could not get collab weights for {session_id}: {e}")

    if weights is None:
        if not global_model:
            raise HTTPException(503, "No model loaded yet. Upload a dataset first.")
        async with model_lock:
            weights = global_model.get_weights()

    buf = io.BytesIO()
    torch.save(weights, buf)
    buf.seek(0)
    return StreamingResponse(
        buf,
        media_type="application/octet-stream",
        headers={
            "Content-Disposition": f"attachment; filename=model_{session_id or 'global'}.pt"
        },
    )


@app.post("/update")
async def receive_update(
    background_tasks: BackgroundTasks,
    client_id: str = Form(...),
    wallet_address: str = Form(default=""),
    model_version: int = Form(default=-1),
    collab_session_id: str = Form(default=""),
    collab_user_id: int = Form(default=0),
    file: UploadFile = File(...),
):
    """
    Receives a client's locally-trained .pt weight file.

    Security checklist (Segment 7):
      ─ File extension must be .pt
      ─ File size must be ≤ 200 MB
      ─ torch.load with weights_only=True (no arbitrary pickle)
      ─ Keys must exactly match global model
      ─ Tensor shapes must match global model

    Version control (Segment 5):
      ─ client model_version must equal current_version_num

    Flow (Segments 2–4):
      1. Security + version checks.
      2. Deserialise + validate state_dict.
      3. Byzantine detection.
      4. REJECT → slash_client (background) + structured response.
      5. ACCEPT → reward_client (background) + enqueue for aggregation.
    """
    global pending_updates

    MAX_PT_BYTES = 200 * 1024 * 1024  # 200 MB hard cap

    # ── Segment 7: extension check ───────────────────────────────────────
    fname = file.filename or ""
    if not fname.lower().endswith(".pt"):
        raise HTTPException(
            400, f"Invalid file type '{fname}'. Only .pt files accepted."
        )

    if not global_model:
        raise HTTPException(503, "No model loaded yet. Upload a dataset first.")

    # ── Segment 5: stale version check ───────────────────────────────
    if model_version >= 0 and model_version != current_version_num:
        return {
            "status": "REJECTED",
            "reason": (
                f"Stale model version: client={model_version}, "
                f"server={current_version_num}. Re-download the global model."
            ),
            "new_model_version": current_version_num,
        }

    # ── Segment 7: size cap + safe torch.load ────────────────────────
    try:
        raw = await file.read()
        if len(raw) > MAX_PT_BYTES:
            raise HTTPException(
                400,
                f"File too large ({len(raw):,} bytes). Max {MAX_PT_BYTES // (1024**2)} MB.",
            )
        client_weights: Dict[str, torch.Tensor] = torch.load(
            io.BytesIO(raw), weights_only=True, map_location="cpu"
        )
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(400, f"Invalid .pt file (could not load): {exc}")

    # ── Segment 7: key + shape validation ──────────────────────────
    if collab_session_id and storage:
        # Load the latest collab shared weights for validation
        global_weights = None
        try:
            r = (
                storage.supabase.table("collab_sessions")
                .select("shared_version_id")
                .eq("id", collab_session_id)
                .single()
                .execute()
            )
            if r.data and r.data.get("shared_version_id"):
                vr = (
                    storage.supabase.table("model_versions")
                    .select("file_path")
                    .eq("id", r.data["shared_version_id"])
                    .single()
                    .execute()
                )
                fp = vr.data.get("file_path") if vr.data else None
                if fp and os.path.exists(fp):
                    global_weights = torch.load(
                        fp, weights_only=True, map_location="cpu"
                    )
        except Exception:
            pass
        if not global_weights:
            async with model_lock:
                global_weights = global_model.get_weights()
        curr_vid = current_version_id or ""
    else:
        async with model_lock:
            global_weights = global_model.get_weights()
            curr_vid = current_version_id or ""

    expected_keys = set(global_weights.keys())
    uploaded_keys = set(client_weights.keys())
    if expected_keys != uploaded_keys:
        missing = expected_keys - uploaded_keys
        extra = uploaded_keys - expected_keys
        raise HTTPException(
            400,
            f"State dict key mismatch. Missing: {missing or None}. Extra: {extra or None}.",
        )
    shape_errors = []
    for k in expected_keys:
        if client_weights[k].shape != global_weights[k].shape:
            shape_errors.append(
                f"{k}: expected {tuple(global_weights[k].shape)}, "
                f"got {tuple(client_weights[k].shape)}"
            )
    if shape_errors:
        raise HTTPException(
            400, f"Tensor shape mismatch: {'; '.join(shape_errors[:5])}"
        )

    # ── Byzantine detection ──────────────────────────────────────────
    status, reason, norm_val, dist_val = detect_update(
        client_weights, global_weights, cfg=runtime_cfg
    )

    # ── DB log (best-effort) ─────────────────────────────────────────
    if storage:
        try:
            db_uuid = storage.register_client(client_id)
            storage.log_client_update(
                curr_vid, db_uuid, status, norm_val, dist_val, reason
            )
        except Exception as db_err:
            logger.warning(f"DB log failed for {client_id}: {db_err}")

    # Always log in-memory (Supabase is secondary)
    _mem_log_client_update(
        client_id, status, norm_val, dist_val, reason, version_id=curr_vid
    )

    # ── Segment 4: blockchain as background task (non-blocking) ─────────
    get_or_create_wallet(client_id)

    if status == "REJECT":
        background_tasks.add_task(slash_client, client_id)  # non-blocking
        logger.warning(f"[Update] REJECT [{client_id}]: {reason}")
        return {
            "status": "REJECTED",
            "reason": reason,
            "new_model_version": current_version_num,
        }

    background_tasks.add_task(reward_client, client_id)  # non-blocking

    if collab_session_id:
        if collab_session_id not in collab_pending_updates:
            collab_pending_updates[collab_session_id] = []
        collab_pending_updates[collab_session_id].append((client_id, client_weights))
        q = len(collab_pending_updates[collab_session_id])
        logger.info(
            f"[Collab] ACCEPT [{client_id}] session={collab_session_id} "
            f"— queue {q}/2"
        )

        # Mark as submitted in DB (avoids frontend having to make a second API call)
        if collab_user_id and storage:
            try:
                r = (
                    storage.supabase.table("collab_sessions")
                    .select("round_submitted")
                    .eq("id", collab_session_id)
                    .single()
                    .execute()
                )
                if r.data:
                    submitted = r.data.get("round_submitted") or []
                    if str(collab_user_id) not in submitted:
                        submitted.append(str(collab_user_id))
                        storage.supabase.table("collab_sessions").update(
                            {"round_submitted": submitted}
                        ).eq("id", collab_session_id).execute()
            except Exception as e:
                logger.error(f"[Collab] mark_submitted failed: {e}")

        if q >= 2:
            batch = list(collab_pending_updates[collab_session_id])
            del collab_pending_updates[collab_session_id]
            background_tasks.add_task(
                _run_aggregation_collab_sync, batch, collab_session_id
            )

        return {
            "status": "ACCEPTED",
            "reason": "Collab update queued.",
            "new_model_version": current_version_num,
        }

    async with model_lock:
        pending_updates.append((client_id, client_weights))
        q = len(pending_updates)
        logger.info(
            f"[Update] ACCEPT [{client_id}] wallet={wallet_address or 'n/a'} "
            f"— queue {q}/{runtime_cfg.MIN_AGGREGATE_SIZE}"
        )
        if q >= runtime_cfg.MIN_AGGREGATE_SIZE:
            batch = list(pending_updates)
            pending_updates.clear()
            background_tasks.add_task(_run_aggregation_sync, batch)

    return {
        "status": "ACCEPTED",
        "reason": "Update queued for aggregation.",
        "new_model_version": current_version_num,
    }


# ---------------------------------------------------------------------------
# Dashboard read endpoints
# ---------------------------------------------------------------------------


@app.get("/metrics")
async def get_metrics():
    """Evaluation + aggregation history for the dashboard.

    Strategy:
      1. Try Supabase — use DB rows when available.
      2. Fall back to in-memory accumulators (_mem_evaluations / _mem_aggregations)
         populated by every aggregation round, regardless of Supabase status.
    This guarantees the dashboard always shows live data after CSV upload.
    """
    eval_rows: List[Dict] = []
    agg_rows: List[Dict] = []

    # ── 1. Try Supabase ───────────────────────────────────────────────────────
    if storage:
        try:
            evals = (
                storage.supabase.table("evaluation_metrics")
                .select("*")
                .order("version_id", desc=True)
                .limit(20)
                .execute()
            )
            aggs = (
                storage.supabase.table("aggregation_logs")
                .select("*")
                .order("version_id", desc=True)
                .limit(20)
                .execute()
            )
            eval_rows = evals.data or []
            agg_rows = aggs.data or []
        except Exception as db_exc:
            logger.warning(
                f"[Metrics] Supabase read failed: {db_exc} — using in-memory cache"
            )

    return {
        "current_version": current_version_num,
        "evaluations": eval_rows,
        "aggregations": agg_rows,
        "pending_queue_size": len(pending_updates),
    }


@app.get("/status")
async def get_status():
    """Returns the current status points to fulfill the prompt"""
    return {
        "status": "online",
        "current_version": current_version_num,
        "dp_enabled": runtime_cfg.DP_ENABLED,
        "aggregation_method": runtime_cfg.AGGREGATION_METHOD,
    }


@app.get("/round-status")
async def get_round_status():
    """Round coordination endpoint (Segment 6).
    Clients call this before training to check if the server is ready
    and to learn the current model version they must train on.
    """
    async with model_lock:
        active_clients = [cid for cid, _ in pending_updates]
        received = len(pending_updates)

    return {
        "current_version": current_version_num,
        "required_updates": runtime_cfg.MIN_AGGREGATE_SIZE,
        "received_updates": received,
        "active_clients": active_clients,
        "round_active": global_model is not None,
    }


@app.get("/clients")
async def get_clients():
    """Recent client update attempts for the dashboard.
    Joins client_updates with clients table to resolve UUID → human-readable name.
    Falls back to in-memory list when Supabase is unavailable or tables are empty.
    """
    db_rows: List[Dict] = []

    if storage:
        try:
            # Join clients to get readable names instead of UUIDs
            rows = (
                storage.supabase.table("client_updates")
                .select(
                    "id,version_id,status,norm_value,distance_value,reason,created_at,clients(client_name)"
                )
                .order("created_at", desc=True)
                .limit(50)
                .execute()
            )
            uid_to_name = {v: k for k, v in storage._client_id_cache.items()}
            for row in rows.data or []:
                row = dict(row)
                # Supabase join returns nested {"clients": {"client_name": "..."}} or None
                nested = row.pop("clients", None)
                if nested and isinstance(nested, dict) and nested.get("client_name"):
                    row["client_id"] = nested["client_name"]
                elif row.get("client_id") in uid_to_name:
                    row["client_id"] = uid_to_name[row["client_id"]]
                db_rows.append(row)
        except Exception as exc:
            logger.warning(f"[Clients] Supabase read failed: {exc}")

    return {"data": db_rows}


@app.get("/versions")
async def get_versions():
    """List all saved global model checkpoints.
    Falls back to in-memory list when Supabase is unavailable or tables are empty.
    """
    db_rows: List[Dict] = []

    if storage:
        try:
            rows = (
                storage.supabase.table("model_versions")
                .select("*")
                .order("version_num", desc=True)
                .limit(50)
                .execute()
            )
            db_rows = rows.data or []
        except Exception as exc:
            logger.error(f"Failed to fetch versions: {exc}")

    return {"data": db_rows}


@app.get("/train-metrics")
async def get_train_metrics():
    """Per-epoch training stats captured from CSV training runs.
    Reads from Supabase first; falls back to in-memory accumulator.
    Used by the Evaluation page's 'Training Dataset' section.
    """
    # Read exclusively from Supabase
    source = []
    if storage:
        try:
            db_rows = storage.read_recent_train_history(300)
            if db_rows:
                source = db_rows
        except Exception as _e:
            logger.warning(f"[/train-metrics] DB read failed: {_e}")

    if not source:
        return {"data": [], "total_epochs": 0}

    # Group by round, sort within each round by epoch
    rounds: Dict[int, List[Dict]] = {}
    for entry in source:
        r = int(entry.get("round", 0))
        rounds.setdefault(r, []).append(entry)

    chart_data = []
    for r in sorted(rounds.keys()):
        for ep in sorted(rounds[r], key=lambda x: int(x.get("epoch", 0))):
            chart_data.append(
                {
                    "label": f"R{r}E{ep['epoch']}",
                    "round": r,
                    "epoch": ep["epoch"],
                    "loss": ep.get("loss"),
                    "accuracy": ep.get("accuracy"),
                    "client_id": ep.get("client_id", ""),
                    "created_at": ep.get("created_at", ""),
                }
            )

    return {"data": chart_data, "total_epochs": len(chart_data)}


@app.get("/model/download")
async def download_global_model(version_id: str = None):
    """Download a specific (or latest) global model checkpoint from Supabase Storage.

    Strategy:
      1. If version_id is given, look up that DB row.
         a. If file_path is a valid bucket key  → download from Storage.
         b. If file_path is a legacy local path → try to read the .pt file
            from disk, upload it to the bucket on-the-fly, update the DB row,
            then stream it back.
      2. If no version_id (latest) → use get_latest_version() which already
         skips legacy local-path rows and returns file_path=None when none
         exist; fall through to in-memory model.
      3. Final fallback: stream the in-memory global model.
    """
    from server.storage import _is_bucket_key  # already imported in storage module

    _BACKEND_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

    def _find_local_pt(raw_path: str) -> str | None:
        """Resolve a raw DB file_path to a real local file path."""
        normalised = raw_path.replace("\\", "/").lstrip("/")
        candidate = os.path.join(_BACKEND_DIR, normalised)
        if os.path.isfile(candidate):
            return candidate
        # Also try fl_simulation_data/models/<basename> directly
        basename = os.path.basename(normalised)
        fallback = os.path.join(_BACKEND_DIR, "fl_simulation_data", "models", basename)
        if os.path.isfile(fallback):
            return fallback
        return None

    def _migrate_and_stream(row_id: str, raw_path: str, version_num: int):
        """Read .pt from disk, upload to bucket, update DB, return (buf, filename)."""
        local_pt = _find_local_pt(raw_path)
        if not local_pt:
            return None, None
        bucket_key = f"global_model_v{version_num}.pt"
        with open(local_pt, "rb") as fh:
            raw_bytes = fh.read()
        try:
            storage.supabase.storage.from_("fl-models").upload(
                path=bucket_key,
                file=raw_bytes,
                file_options={
                    "content-type": "application/octet-stream",
                    "upsert": "true",
                },
            )
            storage.supabase.table("model_versions").update(
                {"file_path": bucket_key}
            ).eq("id", row_id).execute()
            logger.info(f"[Download] Migrated '{raw_path}' → bucket key '{bucket_key}'")
        except Exception as up_exc:
            logger.warning(
                f"[Download] On-the-fly bucket upload failed ({up_exc}); serving from disk."
            )
        buf = io.BytesIO(raw_bytes)
        return buf, bucket_key

    # ── Specific version requested ────────────────────────────────────────────
    if version_id and storage:
        try:
            row_res = (
                storage.supabase.table("model_versions")
                .select("id, version_num, file_path")
                .eq("id", version_id)
                .single()
                .execute()
            )
            row_data = row_res.data
            if not row_data:
                raise HTTPException(404, f"Version '{version_id}' not found.")

            file_name = row_data.get("file_path") or ""
            v_num = row_data.get("version_num", 0)

            if _is_bucket_key(file_name):
                # ✅ Valid bucket key — download normally
                try:
                    state_dict = storage.download_model(file_name)
                    buf = io.BytesIO()
                    torch.save(state_dict, buf)
                    buf.seek(0)
                    return StreamingResponse(
                        buf,
                        media_type="application/octet-stream",
                        headers={
                            "Content-Disposition": f"attachment; filename={file_name}"
                        },
                    )
                except Exception as dl_exc:
                    logger.warning(f"[Download] Bucket download failed: {dl_exc}")
                    # Fall through to disk/memory fallback
            else:
                # ⚠️  Legacy local path — try on-the-fly migration
                buf, bucket_key = _migrate_and_stream(row_data["id"], file_name, v_num)
                if buf is not None:
                    return StreamingResponse(
                        buf,
                        media_type="application/octet-stream",
                        headers={
                            "Content-Disposition": f"attachment; filename={bucket_key or os.path.basename(file_name)}"
                        },
                    )
                logger.warning(
                    f"[Download] Legacy path '{file_name}' not found on disk either."
                )
                # Fall through to in-memory fallback
        except HTTPException:
            raise
        except Exception as exc:
            logger.warning(f"[Download] Specific version lookup failed: {exc}")
            # Fall through to in-memory fallback

    # ── Latest version (no version_id) ───────────────────────────────────────
    elif storage:
        try:
            latest = storage.get_latest_version()  # already skips legacy paths
            file_name = latest.get("file_path")
            if file_name and _is_bucket_key(file_name):
                try:
                    state_dict = storage.download_model(file_name)
                    buf = io.BytesIO()
                    torch.save(state_dict, buf)
                    buf.seek(0)
                    return StreamingResponse(
                        buf,
                        media_type="application/octet-stream",
                        headers={
                            "Content-Disposition": f"attachment; filename={file_name}"
                        },
                    )
                except Exception as dl_exc:
                    logger.warning(
                        f"[Download] Latest bucket download failed: {dl_exc}"
                    )
        except Exception as exc:
            logger.warning(f"[Download] Latest version lookup failed: {exc}")

    # ── Final fallback: in-memory global model ────────────────────────────────
    if global_model:
        try:
            async with model_lock:
                weights = global_model.get_weights()
            buf = io.BytesIO()
            torch.save(weights, buf)
            buf.seek(0)
            logger.info("[Download] Serving in-memory global model as fallback.")
            return StreamingResponse(
                buf,
                media_type="application/octet-stream",
                headers={
                    "Content-Disposition": "attachment; filename=global_model_current.pt"
                },
            )
        except Exception as mm_exc:
            logger.error(f"[Download] In-memory fallback failed: {mm_exc}")

    raise HTTPException(
        404,
        "No model checkpoint available yet. Upload a dataset or run a simulation first.",
    )


# ---------------------------------------------------------------------------
# Simulation  (demo / testing  — fires a real training loop)
# ---------------------------------------------------------------------------


class SimulationRequest(BaseModel):
    client_name: str
    is_malicious: bool = False
    malicious_multiplier: float = cfg.NORM_THRESHOLD * 3
    attack_type: str = "gaussian"  # "gaussian" | "label_flip" | "sign_flip"
    noise_intensity: float = 1.0  # multiplier scaling factor (0.0 – 1.0+)
    label_flip_ratio: float = 1.0  # fraction of labels to flip  (0.0 – 1.0)


@app.post("/simulate")
async def run_simulation(background_tasks: BackgroundTasks, req: SimulationRequest):
    """
    Fires a real local PyTorch training run in a background thread.
    Uses the same global model + FLSettings as real clients.
    Malicious clients inject large Gaussian noise before submission.
    """
    if not global_model:
        raise HTTPException(503, "No model loaded yet. Upload a dataset first.")

    def _worker(
        name: str,
        malicious: bool,
        mult: float,
        vid,
        attack_type: str = "gaussian",
        noise_intensity: float = 1.0,
        label_flip_ratio: float = 1.0,
    ):
        import asyncio as _as
        import random as _rand
        from clients.local_training import get_updated_weights

        logger.info(
            f"[Sim] Starting {name} (attack={attack_type}, malicious={malicious})"
        )
        db_client_id = storage.register_client(f"MOCK-{name}") if storage else name

        gw = _as.run(_async_get_weights())

        # Build a local copy of the global model
        local_m = build_model(
            input_shape=global_model.input_shape,
            num_classes=global_model.num_classes,
            hidden_dims=cfg.hidden_dims(),
        )
        local_m.set_weights(gw)
        local_m.train()

        optimizer = torch.optim.SGD(
            local_m.parameters(), lr=cfg.LEARNING_RATE, momentum=0.9
        )
        criterion = torch.nn.CrossEntropyLoss()

        # Synthetic mini-batches shaped to match the model's input_shape
        c, *spatial = (
            global_model.input_shape
            if len(global_model.input_shape) == 3
            else (global_model.input_shape[0],)
        )
        batch_shape = (16, *global_model.input_shape)
        num_classes = global_model.num_classes

        for _ in range(5):
            inputs = torch.randn(*batch_shape)
            targets = torch.randint(0, num_classes, (16,))

            # --- Label-flip attack: corrupt labels during training ---
            if malicious and attack_type == "label_flip":
                flip_mask = torch.rand(16) < label_flip_ratio
                targets[flip_mask] = (num_classes - 1) - targets[flip_mask]

            optimizer.zero_grad()
            loss = criterion(local_m(inputs), targets)
            loss.backward()
            optimizer.step()

        weights = get_updated_weights(local_m)

        # --- Post-training attack injection ---
        if malicious:
            effective_mult = mult * noise_intensity
            if attack_type == "gaussian":
                for key in weights:
                    weights[key] += torch.randn_like(weights[key]) * effective_mult
            elif attack_type == "sign_flip":
                keys = list(weights.keys())
                n_flip = max(1, int(len(keys) * noise_intensity))
                for key in _rand.sample(keys, min(n_flip, len(keys))):
                    weights[key] = -weights[key]
            # label_flip already applied during training above

        status, reason, norm_val, dist_val = detect_update(weights, gw, cfg=runtime_cfg)
        if storage:
            try:
                storage.log_client_update(
                    vid, db_client_id, status, norm_val, dist_val, reason
                )
            except Exception:
                pass
        # Always log in-memory so dashboard shows simulation results
        _mem_log_client_update(f"MOCK-{name}", status, norm_val, dist_val, reason)

        if status == "ACCEPT":
            _as.run(_enqueue_and_aggregate(db_client_id, weights))
        logger.info(f"[Sim] {name} → {status}")

    background_tasks.add_task(
        _worker,
        req.client_name,
        req.is_malicious,
        req.malicious_multiplier,
        current_version_id,
        req.attack_type,
        req.noise_intensity,
        req.label_flip_ratio,
    )
    return {"status": "Simulation started in background."}


async def _async_get_weights() -> Dict[str, torch.Tensor]:
    async with model_lock:
        return global_model.get_weights()


async def _enqueue_and_aggregate(client_id: str, weights: Dict[str, torch.Tensor]):
    global pending_updates
    async with model_lock:
        pending_updates.append((client_id, weights))
        if len(pending_updates) >= runtime_cfg.MIN_AGGREGATE_SIZE:
            batch = list(pending_updates)
            pending_updates.clear()
            await _aggregate_and_update(batch)


# ---------------------------------------------------------------------------
# Attack probe  (instant preview for Attack Playground sliders)
# ---------------------------------------------------------------------------


class AttackProbeRequest(BaseModel):
    noise_level: float = 100.0
    attack_type: str = "gaussian"  # "gaussian" | "sign_flip"


@app.post("/attack-probe")
async def probe_attack(req: AttackProbeRequest):
    """
    Instant preview: would this noise level get caught by detection.py?
    No training, no state mutation — purely diagnostic for the Attack Playground UI.
    """
    if not global_model:
        raise HTTPException(503, "No model loaded yet. Upload a dataset first.")

    async with model_lock:
        gw = global_model.get_weights()

    # Build synthetic noisy weights
    fake_weights = {}
    if req.attack_type == "sign_flip":
        import random as _rand

        keys = list(gw.keys())
        n_flip = max(1, int(len(keys) * min(req.noise_level / 1000.0, 1.0)))
        for k, v in gw.items():
            fake_weights[k] = v.clone()
        for key in _rand.sample(keys, min(n_flip, len(keys))):
            fake_weights[key] = -fake_weights[key]
    else:
        # gaussian (default)
        for k, v in gw.items():
            fake_weights[k] = v + torch.randn_like(v) * req.noise_level

    status, reason, norm_val, dist_val = detect_update(
        fake_weights, gw, cfg=runtime_cfg
    )

    return {
        "caught": status == "REJECT",
        "status": status,
        "reason": reason,
        "norm": round(norm_val, 2),
        "norm_threshold": runtime_cfg.NORM_THRESHOLD,
        "distance": round(dist_val, 2),
        "distance_threshold": runtime_cfg.DISTANCE_THRESHOLD,
        "slash_amount": 15 if status == "REJECT" else 0,
        "reward_amount": 10 if status == "ACCEPT" else 0,
    }


# ---------------------------------------------------------------------------
# Admin config  (hot-update DP/queue settings without restart)
# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
# Network visualizer endpoint (per-client aggregated summary)
# ---------------------------------------------------------------------------


@app.get("/network")
async def get_network():
    """Aggregated per-client summary for the force-graph network visualizer.
    Returns client nodes with accept/reject counts and last norm/distance,
    plus global aggregation_count and dp_enabled flag.
    """
    # Use in-memory client updates (always populated)
    raw = list(_mem_client_updates)

    # Group by client_id
    client_map: Dict[str, Dict] = {}
    for entry in raw:
        cid = entry.get("client_id", "unknown")
        if cid not in client_map:
            client_map[cid] = {
                "client_id": cid,
                "total_updates": 0,
                "accepted": 0,
                "rejected": 0,
                "last_status": "PENDING",
                "last_norm": 0.0,
                "last_distance": 0.0,
            }
        node = client_map[cid]
        node["total_updates"] += 1
        st = entry.get("status", "")
        if st == "ACCEPT":
            node["accepted"] += 1
        elif st == "REJECT":
            node["rejected"] += 1
        # Always overwrite with the latest values
        node["last_status"] = st
        node["last_norm"] = entry.get("norm_value", 0.0)
        node["last_distance"] = entry.get("distance_value", 0.0)

    return {
        "clients": list(client_map.values()),
        "aggregation_count": len(_mem_aggregations),
        "dp_enabled": runtime_cfg.DP_ENABLED,
    }


class ToggleDPRequest(BaseModel):
    dp_enabled: bool


@app.post("/toggle-dp")
async def toggle_dp(req: ToggleDPRequest):
    """Toggle Differential Privacy dynamically."""
    global runtime_cfg
    runtime_cfg = runtime_cfg.model_copy(update={"DP_ENABLED": req.dp_enabled})
    logger.info(f"Differential privacy toggled to: {req.dp_enabled}")
    return {"status": "ok", "dp_enabled": req.dp_enabled}


class ConfigUpdate(BaseModel):
    dp_enabled: bool = False
    dp_clip_norm: float = 10.0
    dp_noise_mult: float = 1.0
    min_update_queue: int = 1


@app.get("/admin/config")
async def get_config():
    return {
        "dp_enabled": runtime_cfg.DP_ENABLED,
        "dp_clip_norm": runtime_cfg.DP_CLIP_NORM,
        "dp_noise_mult": runtime_cfg.DP_NOISE_MULT,
        "min_update_queue": runtime_cfg.MIN_AGGREGATE_SIZE,
    }


@app.get("/blockchain/status")
async def get_blockchain_economy_status():
    """Live web3 economy stats for real-time dashboard UI, enriched with user info."""
    # Try to fetch additional user info from Supabase if available
    try:
        from server.storage import supabase
        from server.blockchain import get_or_create_wallet

        if supabase:
            # We fetch all users to avoid N queries (fine for a hackathon scale)
            res = supabase.table("users").select("name,email,phone").execute()
            if res.data:
                # Create a lookup map by user name (which matches client_id)
                user_map = {u["name"]: u for u in res.data}

                # AUTO-RESTORE: Uvicorn reloads wipe in-memory wallets.
                # If a user exists in the DB, ensure they have a wallet in memory.
                for uname in user_map.keys():
                    get_or_create_wallet(uname)

                # Now fetch the hydrated wallet list
                wallets = get_all_status()

                # Enrich wallet data
                for w in wallets:
                    cid = w["client_id"]
                    if cid in user_map:
                        w["email"] = user_map[cid].get("email")
                        w["phone"] = user_map[cid].get("phone")

                return {
                    "wallets": wallets,
                    "recent_transactions": get_recent_transactions(),
                }
    except Exception as e:
        logger.error(f"Failed to enrich/restore blockchain status: {e}")

    # Fallback if DB fetch fails
    return {
        "wallets": get_all_status(),
        "recent_transactions": get_recent_transactions(),
    }


@app.post("/admin/config")
async def set_config(update: ConfigUpdate):
    """Hot-updates runtime DP & queue settings in memory (no restart needed)."""
    global runtime_cfg
    # Create a patched copy using model_copy (Pydantic v2)
    runtime_cfg = runtime_cfg.model_copy(
        update={
            "DP_ENABLED": update.dp_enabled,
            "DP_CLIP_NORM": update.dp_clip_norm,
            "DP_NOISE_MULT": update.dp_noise_mult,
            "MIN_AGGREGATE_SIZE": update.min_update_queue,
        }
    )
    logger.info(
        f"[Config] Updated: DP={runtime_cfg.DP_ENABLED}, "
        f"clip={runtime_cfg.DP_CLIP_NORM}, σ={runtime_cfg.DP_NOISE_MULT}, "
        f"queue={runtime_cfg.MIN_AGGREGATE_SIZE}"
    )
    return {"status": "ok"}


# ---------------------------------------------------------------------------
# Dataset upload + per-client weight serving
# ---------------------------------------------------------------------------


def _train_client_background(
    client_id: str, file_path: str, epochs: int = None, version_id: str = None
):
    """
    Background task triggered by CSV upload from the frontend.
    `epochs` overrides cfg.BG_TRAIN_EPOCHS when provided.

    Full pipeline:
      1. Parse CSV → detect input_dim + num_classes automatically.
      2. Build global model if none exists, OR keep the existing one
         but ALWAYS rebuild val_loader from this CSV (critical fix).
      3. Train locally on the uploaded data using real PyTorch.
      4. Submit the trained weights into the aggregation pipeline via
         _run_aggregation_sync (thread-safe, own event loop — avoids
         asyncio.Lock cross-loop deadlock bug).
      5. Aggregate → update global model → evaluate → store metrics.
    """
    import asyncio as _as
    import math as _math

    logger.info(f"[BG Train] Starting for {client_id} on {file_path}")
    try:
        from clients.csv_trainer import (
            _sniff_csv,
            _build_label_map,
            UniversalCSVDataset,
            train_on_csv,
        )
        from torch.utils.data import random_split as _random_split

        # ── Step 1: Detect CSV shape ─────────────────────────────────────────
        has_header, num_features = _sniff_csv(file_path)
        label_map = _build_label_map(file_path, has_header)
        num_classes_csv = len(label_map)
        logger.info(
            f"[BG Train] CSV detected: features={num_features}, "
            f"classes={num_classes_csv}"
        )

        # ── Step 2: Build val_loader from CSV (ALWAYS — not just when model is None) ──
        # This is the critical fix: val_loader must be set from real labeled data
        # every time. If val_loader stays None after MNIST startup fails to load,
        # evaluate_model is never called and evaluation_metrics stays forever empty.
        global global_model, val_loader

        dataset_full = UniversalCSVDataset(file_path, label_map, has_header)
        if len(dataset_full) < 2:
            logger.warning(
                f"[BG Train] Dataset too small ({len(dataset_full)} rows). Need ≥ 2."
            )
            return

        n_val = max(1, int(len(dataset_full) * cfg.TEST_SPLIT_RATIO))
        n_train = len(dataset_full) - n_val
        _, val_subset = _random_split(
            dataset_full,
            [n_train, n_val],
            generator=torch.Generator().manual_seed(42),
        )
        new_val_loader = DataLoader(
            val_subset, batch_size=cfg.BATCH_SIZE, shuffle=False
        )
        logger.info(f"[BG Train] Val loader built: {n_val} samples.")

        if global_model is None:
            # Build model from scratch using detected CSV shape
            side = int(_math.isqrt(num_features))
            is_image = side * side == num_features and side >= 8
            input_shape = (1, side, side) if is_image else (num_features,)

            new_model = build_model(
                input_shape=input_shape,
                num_classes=num_classes_csv,
                hidden_dims=cfg.hidden_dims(),
            )
            logger.info(f"[BG Train] Built global model from CSV: {new_model}")

            # Thread-safe: use a fresh sync event loop to assign under model_lock
            def _set_globals():
                async def _inner():
                    async with model_lock:
                        global global_model, val_loader
                        global_model = new_model
                        val_loader = new_val_loader

                asyncio.run(_inner())

            _set_globals()

            if storage:
                # Save initial version — must use sync loop separate from main
                def _save_init():
                    loop = asyncio.new_event_loop()
                    try:
                        loop.run_until_complete(_init_fresh_model())
                    finally:
                        loop.close()

                _save_init()
        else:
            # Global model already exists — just update val_loader (no lock needed
            # since writes to a module-level reference are GIL-protected in CPython)
            val_loader = new_val_loader
            logger.info("[BG Train] Updated global val_loader from CSV.")

        # ── Step 3: Get current or specific weights ───────────────
        gw = None
        if version_id and storage:
            try:
                logger.info(f"[BG Train] Fetching weights for version {version_id}")
                # Fetch row to get file_path
                res = (
                    storage.supabase.table("model_versions")
                    .select("file_path")
                    .eq("id", version_id)
                    .execute()
                )
                if res.data and res.data[0].get("file_path"):
                    file_path_db = res.data[0]["file_path"]
                    gw = storage.download_model(file_path_db)
                    logger.info(
                        f"[BG Train] Successfully loaded weights for version {version_id}"
                    )
            except Exception as e:
                logger.error(
                    f"[BG Train] Failed to load version {version_id}: {e}. Falling back to global model."
                )

        if gw is None:

            def _get_weights_sync() -> Dict[str, torch.Tensor]:
                async def _inner():
                    async with model_lock:
                        return global_model.get_weights() if global_model else None

                loop = asyncio.new_event_loop()
                try:
                    return loop.run_until_complete(_inner())
                finally:
                    loop.close()

            gw = _get_weights_sync()

        if gw is None:
            logger.error(f"[BG Train] No weights available to train on.")
            return

        # ── Step 4: Train on the CSV ─────────────────────────────────────────
        _epochs = max(1, min(int(epochs), 100000)) if epochs else cfg.BG_TRAIN_EPOCHS
        saved_path, epoch_metrics = train_on_csv(
            client_id=client_id,
            csv_path=file_path,
            global_weights=gw,
            epochs=_epochs,
            device=cfg.DEVICE,
        )

        # Store per-epoch training history in-memory and in Supabase
        if epoch_metrics:
            import datetime as _dt2

            _round_ts = _dt2.datetime.utcnow().isoformat() + "Z"
            for em in epoch_metrics:
                _mem_train_history.append(
                    {
                        "client_id": client_id,
                        "round": current_version_num
                        + 1,  # +1: training precedes aggregation
                        "epoch": em["epoch"],
                        "loss": _safe_float(em["loss"]),
                        "accuracy": _safe_float(em["accuracy"]),
                        "created_at": _round_ts,
                    }
                )
                # Persist to Supabase (best-effort)
                if storage:
                    try:
                        storage.log_train_epoch(
                            client_id=client_id,
                            round_num=current_version_num + 1,
                            epoch=em["epoch"],
                            loss=_safe_float(em["loss"]) or 0.0,
                            accuracy=_safe_float(em["accuracy"]) or 0.0,
                        )
                    except Exception as _te:
                        logger.warning(f"[DB] train_epoch write failed: {_te}")
            if len(_mem_train_history) > 300:
                del _mem_train_history[: len(_mem_train_history) - 300]

        if not saved_path:
            logger.warning(f"[BG Train] ⚠️ No weights produced for {client_id}")
            return

        # ── Step 5: Load the trained weights ─────────────────────────────────
        checkpoint = torch.load(saved_path, weights_only=True, map_location="cpu")
        if isinstance(checkpoint, dict) and "state_dict" in checkpoint:
            client_weights = checkpoint["state_dict"]
        else:
            client_weights = checkpoint

        # ── Step 6: Byzantine detection ──────────────────────────────────────
        from server.detection import detect_update as _detect

        global_weights_snap = _get_weights_sync()
        status, reason, norm_val, dist_val = _detect(
            client_weights, global_weights_snap, cfg=runtime_cfg
        )

        if storage:
            try:
                db_id = storage.register_client(client_id)
                storage.log_client_update(
                    current_version_id or "",
                    db_id,
                    status,
                    norm_val,
                    dist_val,
                    reason,
                )
            except Exception as _db_err:
                logger.warning(f"[BG Train] DB log_client_update failed: {_db_err}")

        # Always log in-memory so the dashboard shows this CSV training client
        _mem_log_client_update(client_id, status, norm_val, dist_val, reason)

        # ── Step 7: Feed into aggregation (thread-safe — own event loop) ─────
        # Background training is server-side (CSV uploaded by the user, trained
        # locally) — it is inherently trusted. We log detection for observability
        # but do NOT block aggregation on it. Byzantine detection is for external
        # untrusted client pushes only (the /update endpoint).
        if status == "REJECT":
            logger.warning(
                f"[BG Train] ℹ️  Detection flagged weights ({reason}) — "
                f"proceeding with aggregation anyway (server-side training is trusted)."
            )
        else:
            logger.info(f"[BG Train] ✅ Detection passed — running aggregation.")

        _run_aggregation_sync([(client_id, client_weights)])

        logger.info(f"[BG Train] ✅ Complete for {client_id}")

    except Exception as exc:
        logger.error(f"[BG Train] ❌ {client_id}: {exc}", exc_info=True)


@app.get("/api/training/status")
async def get_training_status():
    """Lightweight endpoint for the Submit Training Round UI panel.
    Returns how many clients have submitted weights for the current pending
    aggregation queue, and how many are still required to trigger a new round.
    """
    async with model_lock:
        pending_clients = [cid for cid, _ in pending_updates]
        pending_count = len(pending_updates)

    return {
        "current_version": current_version_num,
        "pending_count": pending_count,
        "required_count": runtime_cfg.MIN_AGGREGATE_SIZE,
        "pending_clients": pending_clients,
        "round_active": global_model is not None,
    }


@app.post("/api/dataset/upload")
async def upload_dataset(
    background_tasks: BackgroundTasks,
    client_id: str = Form(...),
    file: UploadFile = File(None),
    dataset_url: str = Form(None),
    epochs: int = Form(None),
    version_id: str = Form(None),
):
    """Upload a dataset (file or URL) and trigger background local training.
    Optional `epochs` form field (1-100000) overrides the default BG_TRAIN_EPOCHS.
    Optional `version_id` specifies which model version to fine-tune from.
    """
    if not file and not dataset_url:
        raise HTTPException(400, "Provide file or dataset_url.")

    # Clamp epochs to safe range
    safe_epochs = max(1, min(int(epochs), 100000)) if epochs else cfg.BG_TRAIN_EPOCHS

    client_dir = os.path.join(
        os.path.abspath(os.path.dirname(__file__)), "..", "data", f"client_{client_id}"
    )
    os.makedirs(client_dir, exist_ok=True)
    file_path = ""

    if file:
        file_path = os.path.join(client_dir, file.filename or "dataset.csv")
        # Stream-write in 1 MB chunks — avoids OOM for large files
        CHUNK = 1024 * 1024
        with open(file_path, "wb") as out:
            while True:
                chunk = await file.read(CHUNK)
                if not chunk:
                    break
                out.write(chunk)
        logger.info(
            f"Dataset '{file.filename}' saved for {client_id} (streaming). epochs={safe_epochs}"
        )

    elif dataset_url:
        parsed = urlparse(dataset_url)
        filename = os.path.basename(parsed.path) or "dataset.csv"
        file_path = os.path.join(client_dir, filename)
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        req = urllib.request.Request(dataset_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, context=ctx) as resp, open(
            file_path, "wb"
        ) as out:
            shutil.copyfileobj(resp, out)
        logger.info(
            f"Dataset downloaded from {dataset_url} for {client_id}. epochs={safe_epochs}"
        )

    background_tasks.add_task(
        _train_client_background, client_id, file_path, safe_epochs, version_id
    )
    return {
        "message": f"Dataset received. Background training started ({safe_epochs} epoch(s)).",
        "epochs": safe_epochs,
    }


@app.get("/api/model/weights/{client_id}")
async def get_client_weights(client_id: str):
    """Download the latest global model weights from Supabase Storage.

    Returns the current global model checkpoint — accessible from any machine.
    Falls back to in-memory global model if no checkpoint exists in the bucket.
    """
    if not storage:
        raise HTTPException(500, "Storage uninitialized.")

    # Try to fetch the latest version from Supabase Storage
    try:
        latest = storage.get_latest_version()
        if latest["file_path"]:
            state_dict = storage.download_model(latest["file_path"])
            buf = io.BytesIO()
            torch.save(state_dict, buf)
            buf.seek(0)
            return StreamingResponse(
                buf,
                media_type="application/octet-stream",
                headers={
                    "Content-Disposition": f"attachment; filename=global_model_v{latest['version_num']}.pt"
                },
            )
    except Exception as exc:
        logger.warning(
            f"[Weights] Supabase download failed ({exc}) — falling back to in-memory model."
        )

    # Fallback: stream the in-memory global model directly
    if global_model:
        async with model_lock:
            weights = global_model.get_weights()
        buf = io.BytesIO()
        torch.save(weights, buf)
        buf.seek(0)
        return StreamingResponse(
            buf,
            media_type="application/octet-stream",
            headers={
                "Content-Disposition": "attachment; filename=global_model_current.pt"
            },
        )

    raise HTTPException(404, "No model available — upload a dataset first.")


from server.collab import collab_router

app.include_router(collab_router, prefix="/collab", tags=["collaboration"])
